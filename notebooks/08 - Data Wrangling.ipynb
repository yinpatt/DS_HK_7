{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://alumni.berkeley.edu/sites/default/files/styles/960x400/public/wranglingbigdata.jpg?itok=k0fK1fJQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> “Show me your code and conceal your data structures, and I shall continue to be mystified. Show me your data structures, and I won’t usually need your code; it’ll be obvious.” \n",
    "\n",
    "<footer>**Eric Raymond**, in The Cathedral and the Bazaar, 1997</footer>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://imgs.xkcd.com/comics/exploits_of_a_mom.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![break](assets/agenda.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Structures : Flat or Go Deep?\n",
    "1. Getting freaky with `pandas`\n",
    "1. Abstracting the query with `blaze`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To  explore data modeling techniques, we have to start with a more or less systematic view of NoSQL data models that preferably reveals trends and interconnections. The following figure depicts imaginary “evolution” of the major NoSQL system families, namely, Key-Value stores, BigTable-style databases, Document databases, Full Text Search Engines, and Graph databases:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://highlyscalable.files.wordpress.com/2012/02/overview2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Serialisation Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When saving or communicating some kind of information, we often use serialization. Serialization takes a Python object and converts it into a string of bytes and vice versa. For example, if you have an object representing information about a user and need to send it over the network, it has to be serialized into a set of bytes that can be pushed over a socket. Then, at the other end, the receiver has to unserialize the object, converting it back into something that Python (or another language) can understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic problem is this: CSVs have inherent schemas. In fact, most of the CSVs that I work with are dumps from a database. While the database can maintain schema information alongside the data, the scheme is lost when serializing to disk. Worse, if the dump is denormalized (a join of two tables), then the relationships are also lost, making it harder to extract entities. Although a header row can give us the names of the fields in the file, it won't give us the type, and there is nothing structural about the serialization format (like there is with JSON) that we can infer the type from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### But we've got pandas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(DATA_DIR + 'funding.csv')\n",
    "print data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The utilities for analysis that `Pandas` gives you, especially `DataFrames`, are extremely useful, and there is obviously a 1:1 relationship between DataFrames and CSV files. We routinely use Pandas for data analyses, quick insights, and even data wrangling of smaller files.\n",
    "\n",
    "The problem is that Pandas is not meant for production-level ingestion or wrangling systems. It is meant for data analysis that can happen completely in memory. As such, when you run this line of code, the entirety of the CSV file is loaded into memory. Likewise, Numpy arrays are also immutable data types that are completely loaded into memory. You've just lost your memory efficiency, especially for larger data sets.\n",
    "\n",
    "And while this may be fine on your laptop, keep in mind that if you're writing data pipeline code, it will probably be run more routinely on a virtual cloud server such as Rackspace, AWS, or Google App Engine. Since you have a budget, it will also probably be running on small or micro servers that might have 1 GB of memory, if you're lucky. You don't want to blow it away!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Built-in CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "def read_funding_data(path):\n",
    "    with open(path, 'rU') as data:\n",
    "        reader = csv.DictReader(data)\n",
    "        for row in reader:\n",
    "            yield row\n",
    "\n",
    "for idx, row in enumerate(read_funding_data(DATA_DIR + 'funding.csv')):\n",
    "    if idx > 10: break\n",
    "    print \"%(company)s (%(numEmps)s employees) raised %(raisedAmt)s on %(fundedDate)s\" % row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of key points with the code above:\n",
    "\n",
    "* Always wrap the `CSV` reader in a function that returns a generator (via the yield statement).\n",
    "*  Open the file in universal newline mode with 'rU' for backwards compatibility.\n",
    "* Use context managers with [callable] as [name] to ensure that the handle to the file is closed automatically.\n",
    "* Use the `csv.DictReader` class only when headers are present, otherwise just use `csv.reader`. (You can pass a list of fieldnames, but you'll see its better just to use a namedtuple as we discuss below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = read_funding_data(DATA_DIR + 'funding.csv')\n",
    "print repr(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll see that the function \"returns\" a generator, thanks to the yield statement in the function definition. This means, among other things, that the data is evaluated lazily. The file is not opened, read, or parsed until you need it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XML is a widely used format for data exchange, because it gives good opportunities to keep the structure in the data and the way files are built on and allows developers to write parts of the documentation in with the data without interfering with the reading of them. This is pretty easy in Python as well. You will need the MiniDom library. It is also preinstalled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x =\"\"\"<data>\n",
    "    <items>\n",
    "        <item name=\"item1\"></item>\n",
    "        <item name=\"item2\"></item>\n",
    "        <item name=\"item3\"></item>\n",
    "        <item name=\"item4\"></item>\n",
    "    </items>\n",
    "</data>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XML is a tree-like structure, while a Pandas DataFrame is a 2D table-like structure. So there is no automatic way to convert between the two. You have to understand the XML structure and know how you want to map its data onto a 2D table. Thus, every XML-to-DataFrame problem is different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from xml.dom import minidom\n",
    "\n",
    "xmldoc = minidom.parseString(x)\n",
    "itemlist = xmldoc.getElementsByTagName('item')\n",
    "\n",
    "print itemlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(itemlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(itemlist[0].attributes['name'].value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for s in itemlist:\n",
    "    print(s.attributes['name'].value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simple analysis, however, we just want a rectangular data-frame with columns and rows and we need to flatten all that structure. The following code does a very simple job of converting an XML file into a Pandas data-frame. It recursively parses every branch in the file creating new columns and storing their value when information is found. It stores not just raw text as variables in the new dataset, but also all of the attributes stored in tags as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def xml2df(xml_doc):\n",
    "    f = open(xml_doc, 'r')\n",
    "    soup = BeautifulSoup(f)\n",
    "    name_list=[]\n",
    "    text_list=[]\n",
    "    attr_list=[]\n",
    "\n",
    "    def recurs(soup):\n",
    "        try:\n",
    "            for j in soup.contents:\n",
    "                try:\n",
    "                    #print j.name\n",
    "                    if j.name!=None:\n",
    "                        name_list.append(j.name)\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    #print j.text\n",
    "                    if j.name!=None:\n",
    "                        #print j.string\n",
    "                        text_list.append(j.string)\n",
    "                except:\n",
    "                    pass\n",
    "                try:\n",
    "                    #print j.attrs\n",
    "                    if j.name!=None:\n",
    "                        attr_list.append(j.attrs)\n",
    "                except:\n",
    "                    pass\n",
    "                recurs(j)\n",
    "        except:\n",
    "            pass\n",
    "    recurs(soup)\n",
    "    attr_names_list = [q.keys() for q in attr_list]\n",
    "    attr_values_list = [q.values() for q in attr_list]\n",
    "    columns = hstack((hstack(name_list),\n",
    "                      hstack(attr_names_list)) )\n",
    "    data = hstack((hstack(text_list),\n",
    "                   hstack(attr_values_list)) )\n",
    "    df = pd.DataFrame(data=matrix(data.T), columns=columns )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON is a simple file format that is very easy for any programming language to read. Its simplicity means that it is generally easier for computers to process than others, such as XML. Working with JSON in Python is almost the same such as working with a Python dictionary. You will need the JSON library, but it is preinstalled to every Python 2.6 and after."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "    \"glossary\": {\n",
    "        \"title\": \"example glossary\",\n",
    "\t\t\"GlossDiv\": {\n",
    "            \"title\": \"S\",\n",
    "\t\t\t\"GlossList\": {\n",
    "                \"GlossEntry\": {\n",
    "                    \"ID\": \"SGML\",\n",
    "\t\t\t\t\t\"SortAs\": \"SGML\",\n",
    "\t\t\t\t\t\"GlossTerm\": \"Standard Generalized Markup Language\",\n",
    "\t\t\t\t\t\"Acronym\": \"SGML\",\n",
    "\t\t\t\t\t\"Abbrev\": \"ISO 8879:1986\",\n",
    "\t\t\t\t\t\"GlossDef\": {\n",
    "                        \"para\": \"A meta-markup language, used to create markup languages such as DocBook.\",\n",
    "\t\t\t\t\t\t\"GlossSeeAlso\": [\"GML\", \"XML\"]\n",
    "                    },\n",
    "\t\t\t\t\t\"GlossSee\": \"markup\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is the same as the `XML`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```xml\n",
    "<!DOCTYPE glossary PUBLIC \"-//OASIS//DTD DocBook V3.1//EN\">\n",
    " <glossary><title>example glossary</title>\n",
    "  <GlossDiv><title>S</title>\n",
    "   <GlossList>\n",
    "    <GlossEntry ID=\"SGML\" SortAs=\"SGML\">\n",
    "     <GlossTerm>Standard Generalized Markup Language</GlossTerm>\n",
    "     <Acronym>SGML</Acronym>\n",
    "     <Abbrev>ISO 8879:1986</Abbrev>\n",
    "     <GlossDef>\n",
    "      <para>A meta-markup language, used to create markup\n",
    "languages such as DocBook.</para>\n",
    "      <GlossSeeAlso OtherTerm=\"GML\">\n",
    "      <GlossSeeAlso OtherTerm=\"XML\">\n",
    "     </GlossDef>\n",
    "     <GlossSee OtherTerm=\"markup\">\n",
    "    </GlossEntry>\n",
    "   </GlossList>\n",
    "  </GlossDiv>\n",
    " </glossary>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be working with JSON later on in today's class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YAML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YAML is a recursive acronym that stands for “YAML Ain’t Markup Language”. It is a serialization format, but it is also (easily) human readable, meaning that it can be used as a configuration language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "yaml.load(\"\"\"\n",
    "# YAML\n",
    "name: Jesse\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple example. Line 1 of the YAML file, or document, is a simple comment. Note that there is a space character right before that # sign. The next line is a simple key value pair which, after being parsed, gets returned to us in a Python dictionary. Simple as pie!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple name-value pair is easy to do. Here is a document with some additional structures and details to try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yaml.load(\"\"\"\n",
    "# YAML\n",
    "object:\n",
    "    attributes:\n",
    "        - attr1\n",
    "        - attr2\n",
    "        - attr3\n",
    "    methods: [ getter, setter ]\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have defined a top-level entity named \"object\". This object has two block mappings related to it, ''attributes'' and ''methods''. The ''attributes'' mapping uses the more verbose YAML syntax for a list, in this case:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "attributes:\n",
    "    - attr1\n",
    "    - attr2\n",
    "    - attr3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an extension treatment of YAML in Python see this [blogpost](http://jessenoller.com/blog/2009/04/13/yaml-aint-markup-language-completely-different)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set your password for this session! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import getpass\n",
    "githubpassword = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from getpass import getpass\n",
    "import json\n",
    "\n",
    "username = 'tijptjik' # Your GitHub username\n",
    "password = githubpassword # Your GitHub password\n",
    "\n",
    "# Note that credentials will be transmitted over a secure SSL connection\n",
    "url = 'https://api.github.com/authorizations'\n",
    "note = 'Data Wrangling with GitHubs'\n",
    "post_data = {'scopes':['repo'],'note': note }\n",
    "\n",
    "response = requests.post(\n",
    "    url,\n",
    "    auth = (username, password),\n",
    "    data = json.dumps(post_data),\n",
    "    )   \n",
    "\n",
    "print \"API response:\", response.text\n",
    "print\n",
    "try : \n",
    "    print \"Your OAuth token is\", response.json()['token']\n",
    "except KeyError:\n",
    "    print response.json()['errors'][0]['code']\n",
    "# Go to https://github.com/settings/applications to revoke this token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making direct HTTP requests to GitHub's API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://avatars.githubusercontent.com/u/15233?v=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "# An unauthenticated request that doesn't contain an ?access_token=xxx query string\n",
    "url = \"https://api.github.com/repos/ga-students/DS_HK_7/stargazers\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Display one stargazer\n",
    "\n",
    "print json.dumps(response.json()[1], indent=1)\n",
    "print\n",
    "\n",
    "# Display headers\n",
    "for (k,v) in response.headers.items():\n",
    "    print k, \"=>\", v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PyGithub to query for stargazers of a particular repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your tokens are listen in the [Github Settings](https://github.com/settings/tokens), if you weren't able to retrieve before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pip install PyGithub\n",
    "from github import Github\n",
    "\n",
    "# XXX: Specify your own access token here\n",
    "\n",
    "ACCESS_TOKEN = '8aa36e64f7fc07384c6edba39ca1ebc2bb08d154'\n",
    "# Specify a username and repository of interest for that user.\n",
    "\n",
    "USER = 'tijptjik'\n",
    "REPO = 'SyntheticSynthesis'\n",
    "\n",
    "client = Github(ACCESS_TOKEN, per_page=100)\n",
    "user = client.get_user(USER)\n",
    "repo = user.get_repo(REPO)\n",
    "\n",
    "# Get a list of people who have bookmarked the repo.\n",
    "# Since you'll get a lazy iterator back, you have to traverse\n",
    "# it if you want to get the total number of stargazers.\n",
    "\n",
    "stargazers = [ s for s in repo.get_stargazers() ]\n",
    "print \"Number of stargazers\", len(stargazers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stargazers[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[x for x in dir(stargazers[0]) if '_' is not x[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image, HTML, display\n",
    "\n",
    "def displayImages(images):\n",
    "    imagesList=''.join( [\"<img style='width: 200px; margin: 0px; float: left; border: 1px solid black;' src='%s' />\" % str(s)     \n",
    "                 for s in images ])\n",
    "    display(HTML(imagesList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "displayImages([img.avatar_url for img in stargazers])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK. I'm not that popular. Let's try out [docopts](https://github.com/docopt/docopts_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "USER = 'docopt'\n",
    "REPO = 'docopts'\n",
    "\n",
    "client = Github(ACCESS_TOKEN, per_page=100)\n",
    "user = client.get_user(USER)\n",
    "repo = user.get_repo(REPO)\n",
    "\n",
    "# Get a list of people who have bookmarked the repo.\n",
    "# Since you'll get a lazy iterator back, you have to traverse\n",
    "# it if you want to get the total number of stargazers.\n",
    "\n",
    "stargazers = [ s for s in repo.get_stargazers() ]\n",
    "print \"Number of stargazers\", len(stargazers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing a trivial property graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Create a directed graph\n",
    "\n",
    "g = nx.DiGraph()\n",
    "\n",
    "# Add an edge to the directed graph from X to Y\n",
    "\n",
    "g.add_edge('X', 'Y')\n",
    "\n",
    "# Print some statistics about the graph\n",
    "\n",
    "print nx.info(g)\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the nodes and edges from the graph\n",
    "\n",
    "print \"Nodes:\", g.nodes()\n",
    "print \"Edges:\", g.edges()\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get node properties\n",
    "\n",
    "print \"X props:\", g.node['X']\n",
    "print \"Y props:\", g.node['Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get edge properties\n",
    "\n",
    "print \"X=>Y props:\", g['X']['Y']\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Update a node property\n",
    "\n",
    "g.node['X'].update({'prop1' : 'value1'})\n",
    "print \"X props:\", g.node['X']\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Update an edge property\n",
    "\n",
    "g['X']['Y'].update({'label' : 'label1'})\n",
    "print \"X=>Y props:\", g['X']['Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dir(repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Expand the initial graph with (interest) edges pointing each\n",
    "# direction for additional people interested. Take care to ensure\n",
    "# that user and repo nodes do not collide by appending their type.\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "USER = 'tijptjik'\n",
    "REPO = 'SyntheticSynthesis'\n",
    "\n",
    "client = Github(ACCESS_TOKEN, per_page=100)\n",
    "user = client.get_user(USER)\n",
    "repo = user.get_repo(REPO)\n",
    "\n",
    "# Get a list of people who have bookmarked the repo.\n",
    "# Since you'll get a lazy iterator back, you have to traverse\n",
    "# it if you want to get the total number of stargazers.\n",
    "\n",
    "stargazers = [ s for s in repo.get_stargazers() ]\n",
    "print \"Number of stargazers\", len(stargazers)\n",
    "\n",
    "g = nx.DiGraph()\n",
    "g.add_node(repo.name, type='repo', owner=user.login)\n",
    "\n",
    "for sg in stargazers[-20:]:\n",
    "    g.add_node(sg.login, type='user')\n",
    "    g.add_edge(sg.login, repo.name, type='gazes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Poke around in the current graph to get a better feel for how NetworkX works\n",
    "print nx.info(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print g.node['SyntheticSynthesis']\n",
    "print g.node['tijptjik']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print g['tijptjik']['SyntheticSynthesis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print g['tijptjik']\n",
    "print g['SyntheticSynthesis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print g.in_edges(['tijptjik'])\n",
    "print g.out_edges(['tijptjik'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print g.in_edges(['SyntheticSynthesis'])\n",
    "print\n",
    "print g.out_edges(['SyntheticSynthesis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from networkx.drawing import draw_spring\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with sns.plotting_context(context=\"poster\"):\n",
    "    pos=nx.spring_layout(g) # positions for all nodes\n",
    "    \n",
    "#     elist=[(u,v) for (u,v,d) in g.edges(data=True)]\n",
    "    \n",
    "    # nodes\n",
    "    nx.draw_networkx_nodes(g,pos, node_cmap=plt.cm.BuPu_r, node_size=10)\n",
    "\n",
    "    # edges\n",
    "    nx.draw_networkx_edges(g,pos,width=1)\n",
    "    \n",
    "    # labels\n",
    "    nx.draw_networkx_labels(g,pos, font_color='b', font_size=18,font_family='Bitstream Vera Sans')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a No-SQL Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following has already been done on the virtual machine.**\n",
    "\n",
    "Head to the [MongoDB download page](http://www.mongodb.org/downloads)  and get the latest stable version) for your OS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### unzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzip `mongodb-X.tgz` and rename folder `mongodb-X` to simply `mongodb`. I normally move this folder to my personal project folder so feel free to move it in wherever you like. Your mongodb folder should have the following structure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "GNU-AGPL-3.0        README              THIRD-PARTY-NOTICES bin\n",
    "mongodb/bin:\n",
    "bsondump     mongod       mongoexport  mongoimport  mongoperf    mongos       mongostat\n",
    "mongo        mongodump    mongofiles   mongooplog   mongorestore mongosniff   mongotop\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### startserver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "cd mongodb\n",
    "./bin/mongod --dbpath . --nojournal &\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Open another terminal windows and type:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "~/YOURPATH/mongodb/bin/mongo\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "MongoDB shell version: 2.4.9\n",
    "connecting to: test\n",
    "Welcome to the MongoDB shell.\n",
    "For interactive help, type \"help\".\n",
    "For more comprehensive documentation, see\n",
    "\thttp://docs.mongodb.org/\n",
    "Questions? Try the support group\n",
    "\thttp://groups.google.com/group/mongodb-user\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!sudo tail -f /var/log/mongodb/mongod.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then you are good to go! Type exit to quit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyMongo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyMongo is a Python distribution containing tools for working with MongoDB, and is the recommended way to work with MongoDB from Python. You can install mongodb from either pip or easy_install.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connecting to MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Pymongo 2.4.1 the Connection() method has been deprecated. Now we most use MongoClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "# Connection to Mongo DB\n",
    "try:\n",
    "    conn=pymongo.MongoClient()\n",
    "    print \"Connected successfully!!!\"\n",
    "except pymongo.errors.ConnectionFailure, e:\n",
    "   print \"Could not connect to MongoDB: %s\" % e \n",
    "conn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mongodb creates databases and collections automatically for you if they don't exist already. A single instance of MongoDB can support multiple independent databases. When working with PyMongo you access databases using attribute style access:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db = conn.tweets\n",
    "db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your database name is such that using attribute style access won’t work (like db-name), you can use dictionary style access instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db = conn['data-science']\n",
    "db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to know what databases are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conn.database_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already created 2 new databases. Why didn't show up with the above command? Well, databases with no collections or with empty collections will not show up with database_names(). Same goes when we try to list empty collections in a database.\n",
    "\n",
    "We'll test it again once we have populate some collections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A collection is a group of documents stored in MongoDB, and can be thought of as roughly the equivalent of a table in a relational database. Getting a collection in PyMongo works the same as getting a database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collection = db.tweets\n",
    "collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db.collection_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However one must be careful when trying to get existing collections. For example, if you have a collection db.user and you type db.usr this is clearly a mistake. Unlike an RDBMS, MongoDB won't protect you from this class of mistake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MongoDB stores structured data as JSON-like documents, using dynamic schemas (called BSON), rather than predefined schemas. An element of data is called a document, and documents are stored in collections. One collection may have any number of documents.\n",
    "\n",
    "Compared to relational databases, we could say collections are like tables, and documents are like records. But there is one big difference: every record in a table has the same fields (with, usually, differing values) in the same order, while each document in a collection can have completely different fields from the other documents.\n",
    "\n",
    "All you really need to know when you're using Python, however, is that documents are Python dictionaries that can have strings as keys and can contain various primitive types (int, float,unicode, datetime) as well as other documents (Python dicts) and arrays (Python lists).\n",
    "\n",
    "To insert some data into MongoDB, all we need to do is create a dict and call .insert() on the collection object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = {\"name\":\"Alberto\",\"surname\":\"Negron\",\"twitter\":\"@Altons\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to insert the above document into a collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collection.insert_one(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conn.database_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db.collection_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations you have created your first document!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now `database_names()` and `collection_names()` show database data-science and collection `tweets`.\n",
    "\n",
    "To recap, we have **databases** containing **collections**. A **collection** is made up of **documents**. Each document is made up of **fields**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !pip install tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the [authentication tutorial](http://docs.tweepy.org/en/v3.2.0/auth_tutorial.html#auth-tutorial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Get Twitter API Key](https://apps.twitter.com/app/new)\n",
    "\n",
    "Be sure to allow your app to also havewrite access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "consumer_key = '1tC8FNuMiVfFa4PvcuizVI2NF'\n",
    "consumer_secret = 'eDrVwrlkl3Qr6F8DaNOhuyOYlt7lSZeifDPo4w3nvi56TPaMPc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "auth_url = auth.get_authorization_url()\n",
    "print 'Please authorize: ' + auth_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PASTE_YOUR_PIN = '6713336'\n",
    "auth.get_access_token(PASTE_YOUR_PIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"ACCESS_KEY =\" , auth.access_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'ACCESS_SECRET =', auth.access_token_secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# access_token = '69573308-BJpz769nKioxfcySpvEBNK7ebU3uS6TuDkMZ6VyBP'\n",
    "# access_token_secret = 'bCFmnvs3QX5HmoE8oTwUHpjE2QIE8xKFRwb06pg2fksJ8'\n",
    "# auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "# If you want to parse continue using the Tweepy object use\n",
    "# api = tweepy.API(auth)\n",
    "# If you just want to pretty print use\n",
    "api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "msg = 'Teaching the Twitter API while MongoDB silently weeps.'\n",
    "response = api.update_status(status=msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=1)\n",
    "pp.pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If you still have the Tweepy Object you can now favorite your  tweet!\n",
    "api.create_favorite(response['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Public tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "api = tweepy.API(auth)\n",
    "public_tweets = api.home_timeline()\n",
    "for tweet in public_tweets:\n",
    "    print tweet.user.name, ':', tweet.text, '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching for a Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweepy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lookup = 'BigData'\n",
    "\n",
    "for status in tweepy.Cursor(api.search, q=lookup).items(10):\n",
    "    # process status here\n",
    "    print status.text, '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = tweepy.Cursor(api.search, q=lookup)\n",
    "results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "items = tweepy.Cursor(api.search, q=lookup).items(1)\n",
    "items?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What can we play with?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for status in tweepy.Cursor(api.search, q=lookup).items(1):\n",
    "    pp.pprint([fn for fn in dir(status) if '_' is not fn[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge : Build up your own dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Search twitter for a specific term, and build up a dataset for it.\n",
    "* Store the following columns:\n",
    "    * `created_at`\n",
    "    * `from_user`\n",
    "    * `from_user_id`\n",
    "    * `from_user_id_str`\n",
    "    * `from_user_name`\n",
    "    * `geo`\n",
    "    * `id`\n",
    "    * `iso_language_code`\n",
    "    * `source`\n",
    "    * `text`\n",
    "    * `to_user`\n",
    "    * `to_user_id`\n",
    "    * `to_user_id_str`\n",
    "    * `to_user_name`\n",
    "* What serialisation format would you use for this data?\n",
    "* Try to index by `created_at`\n",
    "* Change the index to the `datetime` type\n",
    "* Can you do some simple factor analysis based on some of the categorical features?\n",
    "* Write a word count function which counts how often  words show up in tweets about your term.\n",
    "* What are the 10 most popular terms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge : Writing your dataset to MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot of information that we really don't need. Let's keep only the data related to the tweet and insert it into MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define my mongoDB database\n",
    "db = conn.twitter_results\n",
    "# Define my collection where I'll insert my search\n",
    "posts = db.posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# loop through search and insert dictionary into mongoDB\n",
    "for tweet in search:\n",
    "    # Empty dictionary for storing tweet related data\n",
    "    data ={}\n",
    "    \n",
    "    # ADD YOUR DATA STRUCTURE HERE    \n",
    "    \n",
    "    # Insert process\n",
    "    posts.insert(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The _id field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In MongoDB, documents stored in a collection require a unique _id field that acts as a primary key. Because ObjectIds are small, most likely unique, and fast to generate, MongoDB uses ObjectIds as the default value for the _id field if the _id field is not specified; i.e., the mongod adds the _id field and generates a unique ObjectId to assign as its value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading Documents in a Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first and foremost important operation we need to learn is how to retrieve our data from MongoDB. For this, Collections provide the find_one() and find() methods:\n",
    "\n",
    "The find_one() method selects and returns a single document from a collection and returns that document (or None if there are no matches). It is useful when you know there is only one matching document, or are only interested in the first match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posts.find_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get more than a single document as the result of a query we use the find() method. find() returns a Cursor instance, which allows us to iterate over all matching documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posts.find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we can iterate over the first 2 documents (there are a lot in the collection and this is just an example) in the posts collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for d in posts.find()[:2]:\n",
    "    print d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can also use the standard python list():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(posts.find())[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we just want to know how many documents match a query we can perform a count() operation instead of a full query. We can get a count of all of the documents in a collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posts.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query Operators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MongoDB queries are represented as JSON-like structure, just like documents. To build a query, you just need to specify a dictionary with the properties you wish the results to match. For example, this query will match all documents in the posts collection with ISO language code \"en\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posts.find({\"iso_language_code\": \"en\"}).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to retrieve all documents with ISO language code \"en\" and source equal to \"twitterfeed\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posts.find({\"iso_language_code\":\"en\",\"source\":\"twitterfeed\"}).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Queries can also use special query operators. These operators include gt, gte, lt, lte, ne, nin, regex, exists, not, or, and many more. The following queries show the use of some of these operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# cheat: year, month, day, hour, minute, second, microsecond\n",
    "date1 = datetime.strptime(\"18/11/15 18:30\", \"%d/%m/%y %H:%M\")\n",
    "date2 = datetime.strptime(\"18/11/13 18:05\", \"%d/%m/%y %H:%M\")\n",
    "date3 = datetime.strptime(\"18/11/15 18:10\", \"%d/%m/%y %H:%M\")\n",
    "date4 = datetime.strptime(\"18/11/15 18:25\", \"%d/%m/%y %H:%M\")\n",
    "\n",
    "cursor = posts.find({'created_at':{\"$gt\":date1}})\n",
    "cursor.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we will do the same query but add the count() method to only get the count of documents that match the query. We will use count() from now onwards (easier!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posts.find({'created_at':{\"$gt\":date1}}).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tweets dates low than or equal to date2\n",
    "posts.find({'created_at':{\"$lte\":date2}}).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Between 2 dates\n",
    "posts.find({\"created_at\": {\"$gte\": date3, \"$lt\": date4}}).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Posts in either spanish or french using $or\n",
    "posts.find({\"$or\":[{\"iso_language_code\":\"es\"},{\"iso_language_code\":\"fr\"}]}).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All posts except spanish or french\n",
    "posts.find({\"iso_language_code\":{\"$nin\":[\"es\",\"fr\"]}}).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using Regex to find a post with hashtag #analytics\n",
    "import re\n",
    "regex = re.compile(r'#analytics')\n",
    "rstats = posts.find_one({\"text\":regex})\n",
    "rstats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge : Reading your dataset to MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Play around with queries to become comfortable with the syntax, \n",
    "* If `posts.find().sort([(\"created_at\", pymongo.ASCENDING)])` would \n",
    "give you a sorted collection based on 'created_at' field, how would you get the latest 10 tweets?\n",
    "* Write a function which retrieves all the tweets which contain words any of the top 10 most common words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tying is all together : Blaze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Review Code](http://continuum.io/blog/blaze-expressions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [NOSQL DATA MODELING TECHNIQUES](https://highlyscalable.wordpress.com/2012/03/01/nosql-data-modeling-techniques/)\n",
    "* [Simple CSV Data Wrangling with Python](https://districtdatalabs.silvrback.com/simple-csv-data-wrangling-with-python)\n",
    "* [A Python guide for open data file formats](http://opendata.stackexchange.com/questions/1208/a-python-guide-for-open-data-file-formats)\n",
    "* [Dive into Python 3 : XML](http://www.diveintopython3.net/xml.html)\n",
    "*  [Gentle Introduction to MongoDB using Pymongo](http://altons.github.io/python/2013/01/21/gentle-introduction-to-mongodb-using-pymongo/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colofon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "print_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "\n",
    "<link rel=\"stylesheet\" href=\"theme/custom.css\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
